{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PydanticAI -- Agents, Tools & Chains\n",
    "\n",
    "In the previous notebooks, we called the LLM and parsed JSON manually.\n",
    "That works, but it's fragile -- the LLM might return broken JSON, wrong field names, or extra text.\n",
    "\n",
    "**PydanticAI** fixes this. You define a **template** (what the output should look like),\n",
    "and PydanticAI guarantees the LLM fills it in correctly. If it doesn't, it retries automatically.\n",
    "\n",
    "Think of it like a form: you design the form, the LLM fills it in, PydanticAI checks every field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydantic-ai-slim[google]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Your First Agent\n",
    "\n",
    "An **Agent** is a wrapper around the LLM. You create it once, then ask it things.\n",
    "\n",
    "```python\n",
    "agent = Agent(\"model-name\")      # create the agent\n",
    "result = agent.run_sync(\"...\")   # ask it something\n",
    "result.output                     # the answer\n",
    "```\n",
    "\n",
    "That's it. Three lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\"google-gla:gemini-2.0-flash\")\n",
    "result = agent.run_sync(\"What is IFC? One sentence.\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Structured Output (the whole point)\n",
    "\n",
    "Remember manually parsing JSON with `json.loads()`? No more.\n",
    "\n",
    "We define a **model** -- a Python class that describes the shape of the data we want.\n",
    "Each field has a name and a type. PydanticAI forces the LLM to return exactly this shape.\n",
    "\n",
    "```python\n",
    "class DoorCheck(BaseModel):     # <-- this is the \"form\"\n",
    "    door_name: str              # text\n",
    "    width_mm: float             # number\n",
    "    passed: bool                # true or false\n",
    "```\n",
    "\n",
    "Then pass it as `output_type` -- the agent now **must** return a filled-in DoorCheck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class DoorCheck(BaseModel):\n",
    "    door_name: str\n",
    "    width_mm: float\n",
    "    min_required_mm: float\n",
    "    passed: bool\n",
    "\n",
    "agent = Agent(\"google-gla:gemini-2.0-flash\", output_type=DoorCheck)\n",
    "result = agent.run_sync(\"Door A12 is 780mm wide. Minimum is 800mm.\")\n",
    "\n",
    "print(result.output)              # the full object\n",
    "print(f\"Passed? {result.output.passed}\")   # access fields directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `json.loads()`, no stripping markdown fences, no hoping the LLM got the field names right.\n",
    "It just works.\n",
    "\n",
    "---\n",
    "## 3. Chain -- One Agent Feeds Into Another\n",
    "\n",
    "A **chain** = the output of Agent A becomes the input for Agent B.\n",
    "\n",
    "Like an assembly line: one worker extracts the data, the next worker makes the decision.\n",
    "\n",
    "```\n",
    "\"Door is 780mm, min 800mm\"  -->  Agent A (extract)  -->  {rule, actual}  -->  Agent B (decide)  -->  true/false\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract(BaseModel):\n",
    "    rule: str\n",
    "    actual: str\n",
    "\n",
    "extractor = Agent(\"google-gla:gemini-2.0-flash\", output_type=Extract)\n",
    "checker   = Agent(\"google-gla:gemini-2.0-flash\", output_type=bool)\n",
    "\n",
    "# Step 1: extract the facts\n",
    "a = extractor.run_sync(\"Door A12 width is 780mm; min required is 800mm.\")\n",
    "print(f\"Extracted: rule='{a.output.rule}', actual='{a.output.actual}'\")\n",
    "\n",
    "# Step 2: feed into the next agent\n",
    "b = checker.run_sync(f\"Does it pass? rule={a.output.rule} actual={a.output.actual}\")\n",
    "print(f\"Passed: {b.output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tools -- Give the Agent Superpowers\n",
    "\n",
    "A **tool** is a Python function that the agent can call whenever it needs to.\n",
    "The agent reads your question, decides *\"I need to use this tool\"*, calls it, and uses the result.\n",
    "\n",
    "You register a tool using the **`@agent.tool` decorator**.\n",
    "\n",
    "### What's a decorator?\n",
    "\n",
    "If you haven't seen `@something` before -- it's just a tag you put above a function.\n",
    "It means: *\"hey agent, this function exists, you can use it.\"*\n",
    "\n",
    "```python\n",
    "@orchestrator.tool                          # <-- \"register this as a tool\"\n",
    "def check_door(ctx: RunContext[None], ...)   # <-- the function\n",
    "```\n",
    "\n",
    "The `ctx: RunContext[None]` part is just required syntax -- ignore it for now, it's always there.\n",
    "\n",
    "The agent reads the function name and its **docstring** (the text in triple quotes)\n",
    "to understand what the tool does. So write a clear docstring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "orchestrator = Agent(\"google-gla:gemini-2.0-flash\", output_type=str)\n",
    "\n",
    "\n",
    "@orchestrator.tool\n",
    "def check_door_width(ctx: RunContext[None], width_mm: float, min_mm: float) -> str:\n",
    "    \"\"\"Check if a door width meets the minimum requirement.\"\"\"\n",
    "    passed = width_mm >= min_mm\n",
    "    return f\"passed={passed} (width={width_mm}, min={min_mm})\"\n",
    "\n",
    "\n",
    "result = orchestrator.run_sync(\"Is a 780mm door OK if minimum is 800mm?\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened behind the scenes:\n",
    "1. The agent read your question\n",
    "2. It saw it has a tool called `check_door_width`\n",
    "3. It called `check_door_width(width_mm=780, min_mm=800)`\n",
    "4. It got `\"passed=False ...\"` back\n",
    "5. It wrote a human-readable answer using that result\n",
    "\n",
    "**You write the tools. The agent decides when to use them.**\n",
    "\n",
    "---\n",
    "## 5. Putting It Together -- Orchestrator with a Chain as a Tool\n",
    "\n",
    "Now the big one: an agent that can call **other agents** as tools.\n",
    "\n",
    "This is how your compliance checker will work:\n",
    "- The **orchestrator** receives a question\n",
    "- It calls specialized tools (door check, staircase check, etc.)\n",
    "- Each tool can itself use agents internally\n",
    "- The orchestrator combines everything into a final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_agent = Agent(\"google-gla:gemini-2.0-flash\", output_type=str)\n",
    "\n",
    "\n",
    "@smart_agent.tool\n",
    "def run_compliance_check(ctx: RunContext[None], description: str) -> str:\n",
    "    \"\"\"Run a full compliance check on a building element description.\"\"\"\n",
    "    # This tool internally uses the chain from section 3!\n",
    "    a = extractor.run_sync(description).output\n",
    "    ok = checker.run_sync(\n",
    "        f\"Does it pass? rule={a.rule} actual={a.actual}\"\n",
    "    ).output\n",
    "    return f\"passed={ok}, rule='{a.rule}', actual='{a.actual}'\"\n",
    "\n",
    "\n",
    "result = smart_agent.run_sync(\n",
    "    \"Check these two:\\n\"\n",
    "    \"1. Door D1 is 850mm wide, min is 800mm\\n\"\n",
    "    \"2. Door D2 is 720mm wide, min is 800mm\"\n",
    ")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recap\n",
    "\n",
    "| Concept | What it is | One-liner |\n",
    "|---|---|---|\n",
    "| **Agent** | Wrapper around the LLM | `Agent(\"model\")` |\n",
    "| **output_type** | Forces the LLM to return a specific shape | `output_type=MyModel` |\n",
    "| **Chain** | Output of agent A feeds into agent B | Just call two agents in sequence |\n",
    "| **Tool** | A function the agent can call | `@agent.tool` decorator |\n",
    "| **Orchestrator** | An agent whose tools call other agents | Nesting agents inside tools |\n",
    "\n",
    "**This is the pattern for your compliance checker app.** Each check is a tool, the orchestrator calls them all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}